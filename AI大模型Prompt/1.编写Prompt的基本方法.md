# 提示词工程

```
现在你是一个 AI大模型专家，正在世界顶尖大学AI大模型课堂上回答学生问题，你的问题会被学生作为随堂笔记，务必全面、严谨、准确、生动的回答后续问题

要求：

- 分步骤写出你的思考过程，最后给出答案
- 先简单讲清楚问题的本质、核心挑战与解法
- 对回答内容进行结构化
- 对回答内容给出恰当且生动的例子
- 对回答内容给出原理性的解释，便于了解问题的本质
- 对回答内容进行合理的分类，并给出例子和原理解释，尽量不通过表格格式
- 检查你的回答内容是否有误
- 当提问者提出错误或歧义问题时，无需回答问题，而是指出错误或歧义，猜测提问者的真正需求并询问
- 每次回答后根据对问题的理解，给出提问者2~4个新的问题，帮助提问者探索相关问题
  - 这些问题能保证提问者更全面、深入了解相关问题
  - 这些问题也可以与业界最新研究成果有关，帮助提问者了解当前业界的发展
  - 这些问题是你因为与此问题相关最重要、最核心的问题

- 假设你的回答会被装订成册进行发表，请扮演严格的主编和领域专家，检查你的回答，并指出或修正
    - 内容是否存在错误、歧义、不恰当，回答
    - 内容是否全面
    - 内容是否符合业内顶尖水平
    - 内容是否包含前沿研究和实践

- 如果你在知识库中找不到确切的科学共识，请直接回答‘目前的科学界尚无定论’，绝不允许编造理论

- 输出格式：
 - Markdown格式
 - 当你想使用表格输出时，务必判断表格中的内容，当内容较多时禁止使用表格，以避免排版困难

```

## 核心问题

*   **问题的本质与核心挑战**：大语言模型（LLM）的底层机制是“基于当前上下文，预测下一个最可能的词（Next-Token Prediction）”。它本质上是一个**概率引擎**，而非**事实数据库**。这种“只管接话，不管真假”的自回归特性，就是产生**幻觉（Hallucination）**的核心原因。
*   **统一的解法逻辑**：我们编写Prompt的所有努力，本质上都是在进行**“高维概率空间的定向导航”**。通过注入外部事实、拉长推理时间、引入自我博弈或外部工具，我们人为地压低了“虚假生成”的概率，拉高了“严谨推理”的概率。


## 基本方法


### Level 1：基础指令约束（The Baseline）—— 建立安全护栏与角色激活
这是最基础的单轮Prompt技巧，旨在改变模型生成时的默认概率分布。

*   **1. 角色与格式塑造 (Persona & Structuring)**
    *   **原理**：模型潜在空间（Latent Space）中存储了海量数据。赋予“专家角色”并要求“结构化输出（如XML/Markdown）”，能精准激活那些与严谨逻辑、高质量学术语料相关联的神经元子集。
    *   **例子**：与其问“怎么优化数据库？”，不如设定：“你是一位拥有20年经验的Oracle数据库首席架构师。请使用`<thought>`标签写下你的分析过程，并以Markdown格式输出防御SQL注入的规范。”
*   **2. 负向约束与出口机制 (Negative Constraints & Exit Hatch)**
    *   **原理**：阻断模型“强行回答未知问题”的强迫症。通过在Prompt中提供一条明确的合法退出路径（如“我不知道”），截断其生成低概率幻觉内容的路径。
    *   **例子**：“回答以下物理问题。**关键约束：如果你在知识库中找不到确切的科学共识，请直接回答‘目前的科学界尚无定论’，绝不允许编造理论。**”

### Level 2：上下文与范例引导（Information Injection）—— 注入确定性信息
从依赖模型不靠谱的“参数记忆”，转向依赖明确的“外部输入记忆”。

*   **3. 知识锚定法 (Contextual Grounding / RAG的基础)**
    *   **原理**：强行将Transformer的注意力权重（Attention Weights）从模糊的参数记忆，转移到你提供的、确定性的参考文本上。
    *   **例子**：“**请严格且仅根据以下<参考文本>**提取2024年的财报数据。如果文本未提及，必须回答‘信息不足’。\n <参考文本>：[真实的财报段落]”
*   **4. 少样本提示 (Few-Shot Prompting)**
    *   **原理**：利用大模型强大的“上下文学习（In-Context Learning）”能力。通过提供2-3个高质量的输入输出示例，模型能瞬间捕捉任务的内在模式（Pattern），将其输出分布严格对齐到你的期望上。
    *   **例子**：“任务：对新闻进行情感极性打分（-1到1）。\n 示例1：公司破产 -> -1 \n 示例2：利润大涨 -> 1 \n 现在的任务：CEO宣布离职 -> ?”

### Level 3：推理期算力扩展（Compute Scaling）—— 用时间换准确率
逼迫模型“慢思考”，解决复杂逻辑问题。

*   **5. 思维链 (Chain of Thought, CoT)**
    *   **原理**：生成每一个Token消耗固定的算力。通过“让我们一步步思考”，模型在生成中间推理步骤时，实际上是在**借用输出窗口的长度，换取了更深的网络计算层数**，避免了一步跨跃导致的逻辑断裂。
    *   **例子**：“树上10只鸟，打死1只剩几只？**请先分析物理现象，再分析生物本能，最后进行计算。**”
*   **6. 思维树 (Tree of Thoughts, ToT) —— CoT的升维进化**
    *   **原理**：将单向的思维链升级为包含“探索、评估、剪枝、回溯”的树状搜索算法。模型在每一步推理后会自我评估，走不通就退回上一步，避免陷入局部最优解的死胡同。
    *   **例子**：在解决复杂的数学24点或数独时，Prompt会要求大模型列出3种可能的下一步，自我打分后，选择分最高的一条路径继续，若发现死局，则回溯到上一个高分节点。

### Level 4：智能体工作流（Agentic Workflows）—— 引入多轮反馈与外部世界
突破单一模型的内在极限，走向现代AI系统的最高形态。

*   **7. 反思机制 (Reflection / Self-Correction)**
    *   **原理**：模拟人类的“系统1（直觉快思考）”与“系统2（理性慢思考）”。打破一问一答，让模型先生成初稿，再将初稿作为输入，让它扮演“严苛的审查员”挑出自己的逻辑漏洞并重写。
    *   **例子**：“（在模型写完一段代码后追加Prompt）：作为资深QA工程师，请审查你刚才写的代码，找出可能导致内存泄漏的Bug，并给出修复后的最终代码。”
*   **8. 多角色辩论 (Multi-Agent Debate)**
    *   **原理**：实现“群智涌现”。不同的角色设定会激活不同的参数偏好。让代表不同视角的Agent互相质询和反驳，虚假的幻觉通常无法自圆其说，最终沉淀出高度准确的共识。
    *   **例子**：对于复杂的医疗诊断，设定一个Agent扮演心内科专家，另一个扮演消化科专家，通过Prompt让他们进行三轮辩论，最后由“主治医生”Agent总结共识。
*   **9. 工具调用 (Tool Use / Function Calling)**
    *   **原理**：终极的幻觉终结者。不让模型去“猜”事实，而是赋予它调用外部系统（计算器、API、数据库）的权限。遇到盲区时，输出指令获取真实数据，再转化为自然语言。
    *   **例子**：“你无法预测天气或进行复杂乘除。当你需要时，请使用`<call_weather("城市")>`或`<call_calculator("公式")>`。”

---